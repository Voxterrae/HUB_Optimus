> ðŸ‡ªðŸ‡¸ Spanish version: [./es/05_meta_learning.md](./es/05_meta_learning.md)

# Meta-learning

The goal of "meta-learning" is to turn each simulation into concrete improvements to the system: scenario, rules, verification, metrics, and wording. It is not "general reflection"; it is **learning engineering**: detect failures, fix them, and test again.

---

## 1) Outcome in one sentence
- What was actually achieved (not what was attempted)?

## 2) Signals and evidence
List 3â€“10 observable pieces of evidence:
- phrases/text where there was ambiguity
- points where one party could "slip through" due to lack of definition
- moments where verification was impossible
- key concessions that unlocked or blocked progress

## 3) Diagnosis (what failed and why)
Classify the failures (mark all that apply):
- **Ambiguity:** undefined terms, blurry scope, vague deadlines.
- **Weak verification:** no actor, method, or sufficient access.
- **Misaligned incentives:** one side benefits from non-compliance.
- **Wrong sequence:** the order of steps makes the agreement unworkable.
- **Political overload:** internal cost makes it impossible to accept.
- **Spoilers:** external actors or events that break the agreement.
- **Information asymmetry:** one side is negotiating "blind".

## 4) Minimum viable patch (MVP)
Define the smallest change that improves the system:
- "If I could only change ONE thing, I would change: ____"
- How would the corrected text look (1â€“5 lines)?

## 5) Recommended changes (prioritized list)
Make a short list (max. 10), ordered by priority:
1) **High:** the simulation breaks if this is not fixed
2) **Medium:** big improvement, but not blocking
3) **Low:** polishing / style / minor improvements

For each change, specify:
- what changes (file/section)
- why
- how to check that it improved (verifiable criterion)

## 6) Simple metrics (to compare iterations)
Choose 3â€“5 metrics and keep them consistent over time:
- Clarity (0â€“5)
- Verifiability (0â€“5)
- Feasibility (0â€“5)
- Time to first draft (min)
- Number of "open points" at close

## 7) Decision: repeat or scale?
- Repeat the same scenario with patches (iteration)
- Scale to a more complex variant (new scenario)
- Change the approach (different template/rules)

## 8) Log (strongly recommended)
Add at the end of the scenario or in notes:
- Date
- Participants/roles
- Outcome
- Changes applied
- Next experiment

---

## Closing checklist (30 seconds)
- [ ] Is there a final agreement text (even if partial)?
- [ ] Is it clear who verifies and how?
- [ ] Are scope and deadlines clear?
- [ ] Is there a list of open points?
- [ ] Is the "minimum viable patch" written down?

Next:
- Workflow guide: [./README.md](./README.md)
- Scenario template: [./04_scenario_template.md](./04_scenario_template.md)
