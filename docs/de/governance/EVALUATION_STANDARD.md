# HUB_Optimus — Evaluationsstandard

## Zweck
Dieses Dokument definiert das kanonische Format, um Entscheidungen/Ergebnisse systemisch zu bewerten.
Es schützt vor „Optik-Siegen“ und erzwingt Struktur: Anreize, Verifizierbarkeit, Lock-in, Korrekturfester.

## Kernfragen (fix)
1) Reduziert es zukünftiges Risiko?
2) Erhöht es mittel-/langfristige Stabilität?
3) Erzeugt es Lock-in / blockiert es künftige Korrekturen?
4) Korrigiert es Anreize oder verstärkt es Fehlanreize?
5) Ist es verifizierbar (Akteur, Methode, Zugriff, Frequenz)?

## Ergebnis-Typen (operativ)
- **Stabilisierend:** Risiko ↓, Stabilität ↑, verifizierbar, Anreize ausgerichtet.
- **Destabilisierend (maskiert):** wirkt wie Erfolg, erhöht aber Risiko/Fehlanreize.
- **Neutral / transient:** begrenzter Nutzen, stark kontextabhängig.
- **Nicht bewertbar:** Definitionen/Metriken/Mechanismen fehlen.

## Mindeststruktur für eine Evaluation
- **Trigger / Entscheidung / Ereignis**
- **Kontext (kurz)**
- **Anreizkarte (wer wird wofür belohnt/bestraft?)**
- **Verifizierbarkeit (Trust Layer)**
- **Systemische Bewertung (Kernfragen)**
- **Historischer Abgleich (Active Memory, wenn verfügbar)**
- **Kernel-Kohärenzcheck**
- **Korrekturoptionen (wenn Fenster offen)**
- **Klassifikation + Begründung**

## Metriken (minimal)
Wähle 3–5 und halte sie über Iterationen stabil:
- Klarheit (0–5)
- Verifizierbarkeit (0–5)
- Viabilität (0–5)
- Zeit bis Entwurf (Min)
- Offene Punkte (Anzahl)

## Zitierpflicht (Evidenz)
Wenn möglich, referenziere:
- Evidenzklasse (A/B/C),
- Trust-Profil-Dimensionen,
- Datenquelle oder Prüfpfad.

Narrativ ohne Prüfpfad wird nicht hochgestuft.
